# =======================
#      HomePedia Makefile
# =======================

PROJECT_NAME=homepedia
DOCKER=docker compose

# Chemin vers l'app Streamlit
APP_ENTRY=streamlit_app/app.py
# -- Actions globales --

all: stop clean start logs ## Red√©marre tout le projet (stop ‚Üí clean ‚Üí start ‚Üí logs)

build: ## Build toutes les images Docker
	$(DOCKER) build

start: ## D√©marre les conteneurs Docker
	$(DOCKER) up -d

stop: ## Stoppe et supprime les conteneurs
	$(DOCKER) down -v

re: stop start ## Red√©marre les services (√©quivalent √† stop + start)

logs: ## Affiche les logs de tous les services
	$(DOCKER) logs -f

status: ## Affiche l'√©tat des conteneurs
	$(DOCKER) ps

# -- Serveur Spark --

spark-master-start: ## D√©marre le serveur Spark
	$(DOCKER) up -d spark-master

spark-master-stop: ## Stoppe le serveur Spark
	$(DOCKER) down -v spark-master

# -- Spark Worker --

spark-worker-start: ## D√©marre le worker Spark
	$(DOCKER) up -d spark-worker

spark-worker-stop: ## Stoppe le worker Spark
	$(DOCKER) down -v spark-worker

# -- Services --

start-services: ## D√©marre les services
	$(DOCKER) up -d scraper-bienici scraper-seloger scraper-gouv spark-job streamlit

stop-services: ## Stoppe les services
	$(DOCKER) down -v scraper-bienici scraper-seloger scraper-gouv spark-job streamlit

# -- Composants individuels --

run-scraper: ## Lance le scraper principal
	@echo "‚ñ∂ Lancement du scraper..."
	$(DOCKER) exec -it scraper-seloger python scraper/run_seloger_spiders.py
	$(DOCKER) exec -it scraper-bienici python scraper/run_bienici_spiders.py

run-spark: ## Lance le job PySpark principal
	@echo "‚ñ∂ Lancement du job PySpark..."
	$(DOCKER) run --rm spark-job

run-ui: ## Lance l'application Streamlit (UI)
	@echo "‚ñ∂ Lancement de $(PROJECT_NAME) depuis $(APP_ENTRY)..."
	$(DOCKER) up -d streamlit

streamlit-stop: ## Stoppe l'application Streamlit
	$(DOCKER) down -v streamlit
	
# -- Nettoyage --

clean: ## Nettoyage des fichiers g√©n√©r√©s (raw/processed/exports)
	@echo "üßπ Nettoyage des fichiers temporaires..."
	rm -rf data/raw/*
	rm -rf data/processed/*
	rm -rf data/exports/*
	find . -type f -name '*.pyc' -delete
	find . -type d -name '__pycache__' -delete

reset-data: ## Supprime compl√®tement les donn√©es locales (‚ö† destructif)
	@echo "‚ö† Suppression compl√®te des donn√©es locales..."
	rm -rf data/

# -- Debug & DevOps --

attach-scraper: ## Attache au conteneur scraper (manuel)
	docker attach $(PROJECT_NAME)-scraper-1

env-check: ## Affiche les variables de .env
	@echo "‚úÖ V√©rification des variables d‚Äôenvironnement :"
	@cat .env

export-env: ## Charge les variables .env dans l'environnement shell local
	set -a && source .env && set +a

# -- Utilitaires de dev local --

test-pyspark: ## Teste localement preprocessing Spark
	python3 spark_jobs/preprocessing.py

test-ui: ## Lance Streamlit localement hors Docker
	streamlit run streamlit_app/app.py

	lint: ## Ex√©cute flake8 sur le projet
	flake8 streamlit_app tests

	test: ## Lance les tests unitaires
	pytest -q

# -- Meta --

help: ## Affiche cette aide
	@echo ""
	@echo "üì¶ Commandes disponibles :
