# ğŸ  HomePedia â€“ Plateforme dâ€™analyse de donnÃ©es immobiliÃ¨res

**HomePedia** est une application web de data engineering permettant de collecter, traiter et visualiser des donnÃ©es immobiliÃ¨res issues de plusieurs sources (SeLoger, Bien'ici, etc.).  
Son objectif est dâ€™accompagner les utilisateurs dans la prise de dÃ©cision immobiliÃ¨re grÃ¢ce Ã  une analyse claire, interactive et actualisÃ©e.

---

## ğŸš€ Technologies utilisÃ©es

| Composant     | RÃ´le                                         |
|---------------|----------------------------------------------|
| ğŸ•·ï¸ **Scrapy**      | Scraping des annonces immobiliÃ¨res           |
| ğŸ§© **MongoDB**     | Stockage temporaire des donnÃ©es brutes       |
| âš™ï¸ **PySpark**     | Traitement, nettoyage, analyse de donnÃ©es    |
| ğŸ—ƒï¸ **PostgreSQL**  | Base de donnÃ©es relationnelle pour analyse   |
| ğŸ“Š **Streamlit**   | Visualisation interactive de donnÃ©es         |
| ğŸ“¦ **Docker**      | Conteneurisation de tous les services        |

---

## ğŸ§­ Architecture du pipeline

Scrapy (SeLoger, Bien'ici) â”‚ â–¼ MongoDB (brut) â”‚ â–¼ PySpark (nettoyage, analyse) â”‚ â–¼ PostgreSQL (standardisÃ©) â”‚ â–¼ Streamlit (visualisation web)


---

## ğŸ“ Structure du projet

```bash
T-DAT-902-HomePedia/
â”œâ”€â”€ config/                 # Fichiers de configuration globaux (YAML)
â”œâ”€â”€ data/                   # DonnÃ©es locales (non versionnÃ©es)
â”‚   â”œâ”€â”€ raw/                # DonnÃ©es brutes Scrapy
â”‚   â”œâ”€â”€ processed/          # RÃ©sultats Spark
â”‚   â””â”€â”€ exports/            # Export visuel pour Streamlit
â”œâ”€â”€ database/               # SchÃ©ma SQL & collections Mongo
â”œâ”€â”€ docker/                 # Dockerfiles pour chaque service
â”œâ”€â”€ scraper/                # Spiders et scripts Scrapy
â”œâ”€â”€ spark_jobs/            # Nettoyage, analyse, loader PySpark
â”œâ”€â”€ streamlit_app/         # Interface utilisateur Streamlit
â”œâ”€â”€ docker-compose.yml     # Orchestration globale
â”œâ”€â”€ Makefile               # Commandes rapides (build, run, clean)
â”œâ”€â”€ requirements.txt       # DÃ©pendances Python
â””â”€â”€ README.md              # Vous y Ãªtes !


---

## âš™ï¸ Lancer le projet

### PrÃ©requis
- Docker & Docker Compose
- Python 3.10 (optionnel pour exÃ©cutions locales)
- Certaines bibliothÃ¨ques Python nÃ©cessaires Ã  l'interface Streamlit
  (streamlit, streamlit-folium, folium, plotly, geopandas) sont listÃ©es dans
  `requirements.txt`

### Commandes utiles (via Makefile) :

```bash
make build        # Build toutes les images
make start        # Lance tous les services (Attention : peut crÃ©er des problÃ¨mes entre les services)
make stop         # Stoppe les services
make run-ui       # DÃ©marre le conteneur Streamlit
make streamlit-stop   # ArrÃªte le conteneur Streamlit
make clean-all    # Supprime les fichiers de donnÃ©es (data/)
make seed-data    # GÃ©nÃ¨re des donnÃ©es de test JSON
make run-spark    # ExÃ©cute les jobs PySpark (recommendÃ©e aprÃ¨s le scraping)
```


ğŸŒ AccÃ©der Ã  l'application
Lâ€™interface Streamlit est disponible Ã  lâ€™adresse suivante une fois le projet lancÃ© :

ğŸ”— http://localhost:8501


ğŸ“Š FonctionnalitÃ©s prÃ©vues
âœ… Collecte automatisÃ©e dâ€™annonces immobiliÃ¨res (Scrapy)

âœ… Traitement & analyse distribuÃ©e (Spark)

âœ… Visualisation dynamique des tendances prix, rÃ©gions, biens

âœ… Filtres interactifs, cartographies, nuages de mots

âœ… IntÃ©gration PostgreSQL/Mongo + visualisation en direct

## ğŸ”Œ Connexion PostgreSQL pour Streamlit

La partie *streamlit_app* se connecte dÃ©sormais directement Ã  la base
PostgreSQL. Configurez les variables `DB_HOST`, `DB_PORT`, `DB_NAME`,
`DB_USER` et `DB_PASSWORD` (voir `.env.example`) avant de lancer
l'interface pour accÃ©der aux donnÃ©es.
